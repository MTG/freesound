#
# Freesound is (c) MUSIC TECHNOLOGY GROUP, UNIVERSITAT POMPEU FABRA
#
# Freesound is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# Freesound is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Authors:
#     See AUTHORS file.
#

from __future__ import division
from __future__ import print_function

from future import standard_library

standard_library.install_aliases()
from builtins import str
from builtins import range
from builtins import object
from past.utils import old_div
from tagrecommendation_settings import RECOMMENDATION_TMP_DATA_DIR, RECOMMENDATION_DATA_DIR
import sys, os
from utils import saveToJson, mtx2npy, loadFromJson
from numpy import save, load, where, in1d
from math import sqrt
from pysparse import spmatrix
from .community_tag_recommender import CommunityDetector
import datetime


class RecommendationDataProcessor(object):
    '''
    This class has methods to generate all the files that the tag recommendation systems needs to recommend tags.
    To generate these files the data processor needs the Index.json file with the tag association information from freesound.
    The Index.json file must have the following form:

    {
        "1142": [
            "glitch",
            "loop",
            "plucked",
            "string"
        ],
        "1143": [
            "glitch",
            "loop",
            "plucked",
            "string"
        ], ...
    }

    The files that are generated by the system are:
    (for every sound class: Soundscape, Music, Fx, Samples, Speech)
    [[DATABASE]]_[[CLASSNAME]]_SIMILARITY_MATRIX_cosine_SUBSET.npy
    [[DATABASE]]_[[CLASSNAME]]_SIMILARITY_MATRIX_cosine_SUBSET_TAG_NAMES.npy
    '''

    verbose = None

    def __init__(self, verbose=True):
        self.verbose = verbose

    def __repr__(self):
        return "RecommendationDataProcessor instance"

    def tas_to_association_matrix(self, tag_threshold=0, line_limit=1000000000):

        index = loadFromJson(RECOMMENDATION_DATA_DIR + "Index.json")
        # Get tags from file
        ts = []
        idx = 0
        n_original_associations = 0
        sound_ids = []
        if self.verbose:
            print("Reading index file (%i entries)..." % len(index.items()), end=' ')
        for sid, tags in index.items():
            ts += tags
            n_original_associations += len(tags)
            sound_ids.append(sid)

            idx += 1
            if idx > line_limit:
                break

        stats = {
            'n_sounds_in_matrix': len(sound_ids),
            #'biggest_id': max([int(sid) for sid in sound_ids])
        }
        saveToJson(RECOMMENDATION_TMP_DATA_DIR + 'Current_index_stats.json', stats)
        if self.verbose:
            print("done!")

        # Compute tag occurrences after loading the file
        tag_occurrences = dict()
        unique_ts = list(set(ts))
        for id, t in enumerate(unique_ts):
            tag_occurrences[t] = ts.count(t)

            if self.verbose:
                sys.stdout.write("\rComputing tag occurrences %.2f%%"%(float(100*(id+1))/len(unique_ts)))
                sys.stdout.flush()
        print("")
        tags = []
        tags_ids = []
        for id, t in enumerate(unique_ts):

            if tag_occurrences[t] >= tag_threshold:
                tags.append(t)
                tags_ids.append(id)

            if self.verbose:
                sys.stdout.write("\rFiltering tags %.2f%%"%(float(100*(id+1))/len(unique_ts)))
                sys.stdout.flush()

        nTags = len(tags)
        if self.verbose:
            print("")
            print("\tOriginal number of tags: " + str(len(unique_ts)))
            print("\tTags after filtering: " + str(nTags))

        # Generate resource-tags dictionary only with filtered tags
        if self.verbose:
            print("Reading file for resources...", end=' ')
        sys.stdout.flush()
        res_tags = {}
        res_user = {}
        res_tags_no_filt = {}
        idx = 0
        n_filtered_associations = 0
        for sid, stags in index.items():
            resource = sid
            user = None
            assigned_tags = stags
            assigned_tags_filt = list(set(assigned_tags).intersection(set(tags)))
            res_tags_no_filt[resource] = assigned_tags
            res_user[resource] = user
            if len(assigned_tags_filt) > 0:
                res_tags[resource] = assigned_tags_filt
                n_filtered_associations += len(assigned_tags_filt)

            idx += 1
            if idx > line_limit:
                break

        resources = list(res_tags.keys())
        nResources = len(resources)
        resources_ids = list(range(0,nResources))
        if self.verbose:
            print("done!")

        # Generate association matrix
        if self.verbose:
            print("\tOriginal number of associations: " + str(n_original_associations))
            print("\tAssociations after filtering: " + str(n_filtered_associations))

        if self.verbose:
            print('Creating empty array of ' + str(nResources) + ' x ' + str(nTags) + '...', end=' ')
        M = spmatrix.ll_mat(nResources, nTags)
        if self.verbose:
            print('done!')

        done = 0
        for r_id in resources:
            for t in res_tags[r_id]:
                M[resources.index(r_id),tags.index(t)] = 1
                done += 1
                if self.verbose:
                    sys.stdout.write("\rGenerating association matrix %.2f%%" % (float(100*done)/n_filtered_associations))
                    sys.stdout.flush()
        if self.verbose:
            print("")

        # Save data
        if self.verbose:
            print("Saving association matrix, resource ids, tag ids and tag names")

        now = datetime.datetime.now(datetime.timezone.utc)
        filename = "FS%.4i%.2i%.2i" % (now.year, now.month, now.day)
        M.export_mtx(RECOMMENDATION_TMP_DATA_DIR + filename + '_ASSOCIATION_MATRIX.mtx')
        save(RECOMMENDATION_TMP_DATA_DIR + filename + '_RESOURCE_IDS.npy',resources)
        save(RECOMMENDATION_TMP_DATA_DIR + filename + '_TAG_IDS.npy',tags_ids)
        save(RECOMMENDATION_TMP_DATA_DIR + filename + '_TAG_NAMES.npy',tags)
        saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename + '_RESOURCES_TAGS.json',res_tags, verbose = self.verbose)
        #saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename + '_RESOURCES_TAGS_NO_FILTER.json',res_tags_no_filt, verbose = self.verbose)
        #saveToJson(RECOMMENDATION_TMP_DATA_DIR + filename + '_RESOURCES_USER.json',res_user, verbose = self.verbose)

        return filename


    def association_matrix_to_similarity_matrix(self,
                                                metric="cosine",
                                                dataset="FREESOUND",
                                                save_sim=False,
                                                training_set=None,
                                                out_name_prefix="",
                                                is_general_recommender=False):

        if self.verbose:
            print("Loading association matrix and tag names, ids files...")
        try:
            M = spmatrix.ll_mat_from_mtx(RECOMMENDATION_TMP_DATA_DIR + dataset + "_ASSOCIATION_MATRIX.mtx")
            resource_ids = load(RECOMMENDATION_TMP_DATA_DIR + dataset + "_RESOURCE_IDS.npy")
            tag_names = load(RECOMMENDATION_TMP_DATA_DIR + dataset + "_TAG_NAMES.npy")
        except Exception:
            raise Exception("Error loading association matrix and tag names, ids data")

        if metric not in ['cosine', 'binary', 'coocurrence', 'jaccard']:
            raise Exception("Wrong similarity metric specified")

        if self.verbose:
            print("Computing similarity matrix from a resource subset of the whole association matrix...")
        # Get index of resources to train (usable index for M)
        resource_id_positions = where(in1d(resource_ids, training_set, assume_unique=True))[0]

        # Matrix multiplication (only taking in account resources in training set and ALL tags)
        MM = spmatrix.dot(M[resource_id_positions, :], M[resource_id_positions, :])

        # Get similarity matrix
        sim_matrix = spmatrix.ll_mat(MM.shape[0],MM.shape[0])
        non_zero_index = list(MM.keys())
        for index in non_zero_index:
            if metric == 'cosine':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]] * (old_div(1, (sqrt(MM[index[0], index[0]]) * sqrt(MM[index[1], index[1]]))))
            elif metric == 'coocurrence':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]]
            elif metric == 'binary':
                sim_matrix[index[0], index[1]] = old_div(MM[index[0], index[1]],MM[index[0], index[1]])
            elif metric == 'jaccard':
                sim_matrix[index[0], index[1]] = MM[index[0], index[1]] * (old_div(1, (MM[index[0], index[0]] + MM[index[1], index[1]] - MM[index[0], index[1]])))

        # Clean out similarity matrix (clean tags that are not used)
        tag_positions = []
        for i in range(0, sim_matrix.shape[0]):
            if sim_matrix[i, i] != 0.0:
                tag_positions.append(i)

        # Transform sparse similarity matrix to npy format
        sim_matrix_npy = mtx2npy(sim_matrix[tag_positions,tag_positions])
        tag_names_sim_matrix = tag_names[tag_positions]

        if save_sim:
            if not is_general_recommender:
                # Save sim
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_%s_SIMILARITY_MATRIX_" % out_name_prefix + metric + "_SUBSET.npy"
                if self.verbose:
                    print("Saving to " + path + "...")
                save(path, sim_matrix_npy)

                # Save tag names
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_%s_SIMILARITY_MATRIX_" % out_name_prefix + metric + "_SUBSET_TAG_NAMES.npy"
                if self.verbose:
                    print("Saving to " + path + "...")
                save(path, tag_names_sim_matrix)
            else:
                # Save sim
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_SIMILARITY_MATRIX_" + metric + ".npy"
                if self.verbose:
                    print("Saving to " + path + "...")
                save(path, sim_matrix_npy)

                # Save tag names
                path = RECOMMENDATION_TMP_DATA_DIR + dataset + "_SIMILARITY_MATRIX_" + metric + "_TAG_NAMES.npy"
                if self.verbose:
                    print("Saving to " + path + "...")
                save(path, tag_names_sim_matrix)

        return {'SIMILARITY_MATRIX': sim_matrix_npy, 'TAG_NAMES': tag_names_sim_matrix}

    def process_tag_recommendation_data(self,
                                        resources_limit=None,
                                        tag_threshold=10,
                                        line_limit=99999999999999,
                                        recompute_all_classes=False,
                                        similarity_metric="cosine"):

        # Process tas file and turn into association matrix and derived files
        database_name = self.tas_to_association_matrix(tag_threshold=tag_threshold, line_limit=line_limit)

        print("Loading community detector...")
        cd = CommunityDetector(verbose=False, PATH=RECOMMENDATION_DATA_DIR + "Classifier")
        print(cd)

        # Classify existing resources
        resources_tags = loadFromJson(RECOMMENDATION_TMP_DATA_DIR + database_name + '_RESOURCES_TAGS.json')
        instances_ids = list(resources_tags.keys())
        try:
            resource_class = loadFromJson(RECOMMENDATION_DATA_DIR + 'Classifier_classified_resources.json')
        except Exception as e:
            resource_class = dict()

        for count, id in enumerate(instances_ids):
            if not recompute_all_classes:
                if id not in resource_class:
                    resource_class[id] = cd.detectCommunity(input_tags=resources_tags[id])
            else:
                resource_class[id] = cd.detectCommunity(input_tags=resources_tags[id])

            if self.verbose:
                sys.stdout.write("\rClassifying resources... %.2f%%"%(float(100*(count+1))/len(instances_ids)))
                sys.stdout.flush()

        print("")
        saveToJson(RECOMMENDATION_DATA_DIR + 'Classifier_classified_resources.json', resource_class)
        print("")

        print("\nComputing data for general recommender...")
        self.association_matrix_to_similarity_matrix(
            dataset=database_name,
            training_set=instances_ids[0:resources_limit],
            save_sim=True,
            is_general_recommender=True,
            metric=similarity_metric,
        )

        print("\nComputing data for class recommenders...")
        instance_id_class = []
        distinct_classes = []
        for count, instance_id in enumerate(instances_ids):
            class_id = resource_class[instance_id]
            instance_id_class.append([instance_id, class_id])

            if class_id not in distinct_classes:
                distinct_classes.append(class_id)

        print(distinct_classes)

        for collection_id in distinct_classes:
            print("\nComputing recommender for collection %s..." % collection_id)

            # All resources from the training set classified as the selected category
            # (instead of all manually labeled)
            training_ids = []
            for instance in instance_id_class:
                if instance[1] == collection_id:
                    training_ids.append(instance[0])
            # Add limit
            training_ids = training_ids[0:resources_limit]

            if len(training_ids) < 1:
                raise Exception("Too less training ids for collection %s" % collection_id)

            self.association_matrix_to_similarity_matrix(
                dataset=database_name,
                training_set=training_ids,
                save_sim=True,
                out_name_prefix=collection_id,
                is_general_recommender=False,
                metric=similarity_metric,
            )

    def clear_temp_files(self):

        new_data = False
        for filename in os.listdir(RECOMMENDATION_TMP_DATA_DIR):
            if "SIMILARITY_MATRIX" in filename and "SUBSET" in filename:
                new_data = True
                break

        if not new_data:
            raise Exception("There is no new matrix data to update the tag recommendation system")

        for filename in os.listdir(RECOMMENDATION_DATA_DIR):
            file_extension = filename.split(".")[-1]
            if file_extension in ['npy', 'json', 'pkl']:
                if "Classifier" not in filename and "Index" not in filename:  # Do not alter Classifier files
                    if filename[0:6] == "backup":
                        # Delete old backups
                        print("Removing %s" % RECOMMENDATION_DATA_DIR + filename)
                        os.remove(RECOMMENDATION_DATA_DIR + filename)
                    else:
                        # Set previous matrix to "backup mode" (will be deleted in the next update)
                        print("Setting to backup %s" % RECOMMENDATION_DATA_DIR + filename)
                        os.rename(RECOMMENDATION_DATA_DIR + filename, RECOMMENDATION_DATA_DIR + "backup_" + filename)

        current_database_name = ""
        class_names = []
        for filename in os.listdir(RECOMMENDATION_TMP_DATA_DIR):
            file_extension = filename.split(".")[-1]
            if "Index" not in filename:
                if ("SIMILARITY_MATRIX" in filename and "SUBSET" in filename) or "stats" in filename:
                    # Move similarity matrix to recommendation data dir
                    print("Moving %s" % RECOMMENDATION_TMP_DATA_DIR + filename)
                    os.rename(RECOMMENDATION_TMP_DATA_DIR + filename, RECOMMENDATION_DATA_DIR + filename)
                    if "stats" not in filename:
                        current_database_name = filename.split("_")[0]
                        class_names.append(filename.split("_")[1])
                else:
                    # Remove remaining files in tmp dir (except for the tas file)
                    print("Clearing %s" % RECOMMENDATION_TMP_DATA_DIR + filename)
                    os.remove(RECOMMENDATION_TMP_DATA_DIR + filename)

        class_names = list(set(class_names))
        saveToJson(RECOMMENDATION_DATA_DIR + 'Current_database_and_class_names.json', {'database': current_database_name, 'classes':class_names})

        # NOTE: after the cleaning, tag recommendation needs to be reloaded manually

    def rollback_last_backup(self):
        backup_data = False
        for filename in os.listdir(RECOMMENDATION_DATA_DIR):
            if "backup" in filename:
                backup_data = True
                break

        if not backup_data:
            raise Exception("There is no backup data to roll back")

        for filename in os.listdir(RECOMMENDATION_DATA_DIR):
            file_extension = filename.split(".")[-1]
            if file_extension in ['npy', 'json']:
                if "Classifier" not in filename and "Index" not in filename:  # Do not alter Classifier files and index
                    if filename[0:6] != "backup":
                        print("Removing %s" % RECOMMENDATION_DATA_DIR + filename)
                        os.remove(RECOMMENDATION_DATA_DIR + filename)

        for filename in os.listdir(RECOMMENDATION_DATA_DIR):
            file_extension = filename.split(".")[-1]
            if file_extension in ['npy', 'json']:
                if "Classifier" not in filename:  # Do not alter Classifier files
                    if filename[0:6] == "backup":
                        # Set previous matrixes to "backup mode" (will be deleted in the next update)
                        print("Rolling back backup %s" % RECOMMENDATION_DATA_DIR + filename)
                        os.rename(RECOMMENDATION_DATA_DIR + filename, RECOMMENDATION_DATA_DIR + filename[7:])

        # NOTE: after the rollback, tag recommendation needs to be reloaded manually
